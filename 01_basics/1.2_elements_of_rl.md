# 1.2 强化学习的基本要素

在一个标准的强化学习问题中，我们通常用一个马尔可夫决策过程（MDP）来描述：

- 状态空间 $\mathcal{S}$：所有可能的环境状态 $s$
- 动作空间 $\mathcal{A}$：智能体可以采取的动作 $a$
- 状态转移概率 $P(s'|s,a)$：在状态 $s$ 采取动作 $a$ 后转移到 $s'$ 的概率
- 奖励函数 $R(s,a)$：在状态 $s$ 采取动作 $a$ 后得到的立即奖励 $r$
- 折扣因子 $\gamma \in [0,1)$：控制对未来奖励的重视程度

## 1.2.1 轨迹与回报

一次从起始到结束的交互产生一条轨迹（trajectory）：

$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)
$$

我们定义从时间步 $t$ 开始的**折扣回报**（Return）为：

> $$
G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots
$$

> 直观理解：$G_t$ 表示“从现在开始往后看，未来能赚到的总收益”。

---

## 1.2.2 策略（Policy）

**策略** $\pi$ 决定了智能体在每个状态 $s$ 下如何选择动作 $a$。

- **随机策略**：$\pi(a|s)$ 是一个条件概率分布，表示在状态 $s$ 下采取动作 $a$ 的概率；
- **确定性策略**：$a = \mu(s)$，每个状态对应一个唯一动作。

> 我们所有的目标，都是在“选择一个好的策略 $\pi$”。

---

## 1.2.3 价值函数（Value Function）的核心概念

为了评估一个策略是否“好”，我们需要一个衡量标准，这就是**价值函数**。

### 1. 状态价值函数 $V^\pi(s)$ —— V 值

在策略 $\pi$ 下，从状态 $s$ 出发，未来能获得的期望折扣回报：

> $$
V^\pi(s) = \mathbb{E}_\pi[ G_t \mid s_t = s ]
= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} \mid s_t = s \right]
$$

直观理解：

> “如果我现在在状态 $s$，然后一直按照策略 $\pi$ 行动，未来整体表现有多好？”

V 值只看“当前状态”，不关心此刻要选的具体动作。

---

### 2. 动作价值函数 $Q^\pi(s,a)$ —— Q 值

在策略 $\pi$ 下，从状态 $s$ 出发、**先执行一个特定动作 $a$**，再按照策略 $\pi$ 行动，未来能获得的期望折扣回报：

> $$
Q^\pi(s,a) = \mathbb{E}_\pi[ G_t \mid s_t = s, a_t = a ]
$$

直观理解：

> “如果我在状态 $s$，现在先选动作 $a$，之后再按照策略 $\pi$，未来整体表现有多好？”

Q 值同时依赖状态和动作，是 Value-based 方法（如 Q-learning、DQN）的核心目标。

---

### 3. V 值与 Q 值的关系

在给定策略 $\pi$ 的情况下，V 值和 Q 值有非常紧密的关系：

> 1. **V 是 Q 的加权平均**（按策略选择动作的概率）：

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi(\cdot|s)}[Q^\pi(s,a)]
= \sum_a \pi(a|s) Q^\pi(s,a)
$$

> 2. 对于最优情况（最优策略 $\pi^*$ 与最优价值函数 $V^*, Q^*$），有：

$$
V^*(s) = \max_a Q^*(s,a)
$$

直观理解：  
- Q 告诉你“在状态 $s$ 下，选每个动作 $a$ 各自有多好”；  
- V 则是 “在状态 $s$ 下，按策略整体来说期望表现如何”。

---

## 1.2.4 优势函数（Advantage Function）

**优势函数** $A^\pi(s,a)$ 定义为：

$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$

直观理解：

> “在状态 $s$ 下，这个具体动作 $a$ 相比策略的平均水平，是更好还是更差？”

- $A^\pi(s,a) > 0$：动作 $a$ 比当前策略在该状态下的平均水平更优；
- $A^\pi(s,a) < 0$：动作 $a$ 比平均水平更差。

> 优势函数在策略梯度中起到“减小方差”的作用，也更符合人类直觉：我们关心的是“比平均情况好多少”。

---

## 1.2.5 Bellman 方程

价值函数满足著名的 **Bellman 方程**:

对 $V^\pi$：

> $$
V^\pi(s)
= \mathbb{E}_{a\sim\pi(\cdot|s)} \left[ R(s,a) + \gamma \mathbb{E}_{s'\sim P(\cdot|s,a)}[ V^\pi(s') ] \right]
$$

对 $Q^\pi$：

> $$
Q^\pi(s,a)
= R(s,a) + \gamma \mathbb{E}_{s'\sim P(\cdot|s,a), a'\sim\pi(\cdot|s')}[ Q^\pi(s',a') ]
$$

这些方程是后续很多算法（如动态规划、Q-learning、DQN）的理论基础，会在 `1.4_markov_decision_process.md` 以及后续章节中详细展开。

---

## 小结

本节你需要重点记住几个核心概念：

- **回报 $G_t$**：从当前时刻起，未来累积奖励的折扣和；
- **状态价值函数 $V^\pi(s)$（V 值）**：在策略 $\pi$ 下，状态 $s$ 的“好坏”；
- **动作价值函数 $Q^\pi(s,a)$（Q 值）**：在状态 $s$ 下，选动作 $a$ 的“好坏”；
- **优势函数 $A^\pi(s,a)$**：当前动作相对平均水平“好多少”。

这几个量会贯穿整个强化学习教程，是理解后续所有算法（Q-learning / DQN / PG / A2C / PPO / SAC / RLPD 等）的基础。
