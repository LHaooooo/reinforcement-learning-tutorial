# 2.1 马尔可夫决策过程（MDP）与 Bellman 方程

本节详细展开 MDP 的形式化定义和 Bellman 方程，是后续所有价值迭代、策略迭代以及 Q-learning/DQN 等算法的基础。

## 2.1.1 MDP 的正式定义

一个 MDP 由五元组 $\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$ 构成：
- **状态空间** $\mathcal{S}$：可能的环境状态集合；
- **动作空间** $\mathcal{A}$：智能体可选动作集合；
- **状态转移概率** $P(s'|s,a)$：在状态 $s$ 采取动作 $a$ 后转移到 $s'$ 的概率；
- **奖励函数** $R(s,a)$ 或 $R(s,a,s')$：执行动作后的即时奖励期望；
- **折扣因子** $\gamma \in [0,1)$：权衡当前与未来奖励的相对重要性。

**马尔可夫性**：未来只依赖当前状态和动作，与更早历史无关。

## 2.1.2 轨迹与目标

一条轨迹：$\tau = (s_0,a_0,r_0,s_1,a_1,r_1,\dots)$  
> 目标：最大化期望折扣回报 $\mathbb{E}_\pi\left[\sum_{t=0}^{\infty} \gamma^t r_t\right]$。

## 2.1.3 价值函数与 Bellman 方程

**状态价值函数**（给定策略 $\pi$）：
$$
V^\pi(s) = \mathbb{E}_\pi\Big[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\Big]
$$

**动作价值函数**：
$$
Q^\pi(s,a) = \mathbb{E}_\pi\Big[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a\Big]
$$

二者满足 **Bellman 方程**：
> $$
V^\pi(s) = \mathbb{E}_{a\sim\pi(\cdot|s),\,s'\sim P(\cdot|s,a)}\big[R(s,a) + \gamma V^\pi(s')\big]
$$
> $$
Q^\pi(s,a) = \mathbb{E}_{s'\sim P(\cdot|s,a),\,a'\sim\pi(\cdot|s')}\big[R(s,a) + \gamma Q^\pi(s',a')\big]
$$

## 2.1.4 最优性与 Bellman Optimality

最优价值：
$$
V^*(s) = \max_\pi V^\pi(s), \quad Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

> 最优价值满足 **Bellman Optimality 方程**：
$$
V^*(s) = \max_a \mathbb{E}_{s'\sim P(\cdot|s,a)}\big[R(s,a) + \gamma V^*(s')\big]
$$
$$
Q^*(s,a) = \mathbb{E}_{s'\sim P(\cdot|s,a)}\big[R(s,a) + \gamma \max_{a'} Q^*(s',a')\big]
$$

> 从 $Q^*$ 可直接导出最优策略：
$$
\pi^*(s) = \arg\max_a Q^*(s,a)
$$

## 2.1.5 动态规划（表格型）

> 思想：利用**已知模型** $P,R$ 和可枚举的有限状态集合，直接在表格上反复套用 Bellman 方程。

典型流程：

1) **策略评估 (Policy Evaluation)**  
   固定策略 $\pi$，迭代更新  
   $$
   V_{k+1}(s)=\sum_a \pi(a|s)\sum_{s'}P(s'|s,a)\big[R(s,a)+\gamma V_k(s')\big]
   $$
   直到所有状态的变化 $\max_s|V_{k+1}(s)-V_k(s)|<\varepsilon$。

2) **策略改进 (Policy Improvement)**  
   用最新 $V$ 对每个状态贪心选择动作：  
   $$
   \pi_{\text{new}}(s)=\arg\max_a \sum_{s'}P(s'|s,a)\big[R(s,a)+\gamma V(s')\big]
   $$

3) **策略迭代 (Policy Iteration)**  
   反复执行 “评估→改进”，通常有限轮次即可收敛到最优策略。

4) **价值迭代 (Value Iteration)**  
   把 1) 与 2) 合并成单步更新：  
   $$
   V_{k+1}(s)=\max_a \sum_{s'}P(s'|s,a)\big[R(s,a)+\gamma V_k(s')\big]
   $$
   每次更新后可顺便导出当前贪心策略。

**适用前提**：模型可用、状态动作空间足够小。高维或连续时，需用函数逼近（DQN/Actor-Critic）替代显式表格。

## 2.1.6 折扣 vs 平均回报

- **折扣回报**（本教程默认）：确保无限序列的和收敛，强调近期奖励。
- **平均回报**：在持续任务中有时用长期平均 $ \lim_{T\to\infty} \frac{1}{T}\sum_{t=0}^{T-1} r_t $，更新公式和最优性方程略有不同。

## 2.1.7 小结

- MDP 给出了 RL 问题的数学框架；
- 马尔可夫性使得 Bellman 分解成立。
- Bellman 方程/最优性方程是后续 Value-based（Q-learning/DQN/TD3）和 Actor-Critic（A2C/PPO/SAC）算法的核心基石。
- 表格型 DP 方法奠定了价值迭代思想，深度 RL 则是在高维状态下用神经网络近似这些价值/策略函数。
