# 1.3 强化学习算法的分类

这里从多个角度给出常见的 RL 算法分类。

## 1.3.1 按学习对象
- **Value-based**：直接学习状态/动作价值 $V(s)$ 或 $Q(s,a)$，利用贪心或 $\epsilon$-贪心导出策略。代表：Q-learning、DQN、Double DQN、Dueling DQN、Rainbow。
- **Policy-based**：直接对策略参数化，最大化期望回报。代表：REINFORCE、(Stochastic/Deterministic) Policy Gradient。
- **Actor-Critic**：同时学习策略（Actor）和价值函数（Critic），用 Critic 降低策略梯度方差。代表：A2C/A3C、PPO、TRPO、DDPG、TD3、SAC。

## 1.3.2 按数据使用方式
- **On-policy**：只能用当前策略采样的数据来更新同一策略，样本不复用或复用有限（如 PPO 的多次 epoch）。代表：REINFORCE、A2C、PPO、TRPO。
- **Off-policy**：可以使用“旧策略”或“别的策略”采样的经验，通过重要性采样或其他修正实现。代表：Q-learning、DQN 系列、DDPG、TD3、SAC。

## 1.3.3 按动作空间
- **离散动作**：动作有限且可枚举，如 CartPole。常用 DQN、表格 Q-learning。
- **连续动作**：动作为实数向量，如 Pendulum、机器人控制。常用 DDPG、TD3、SAC、PPO（高斯策略）。

## 1.3.4 按模型使用
- **Model-free**：不显式学习环境动力学 $P(s'|s,a)$，直接学值或策略（本教程主要覆盖）。
- **Model-based**：显式学习或利用环境模型做规划或辅助（如 MPC、Dyna、MBPO、Dreamer）。能提升样本效率，但实现复杂。

## 1.3.5 按任务类型
- **Episodic**：有明确终止（如游戏通关、失败或时间上限）。
- **Continuing**：无自然终点（如在线推荐、工业控制），常用平均回报或折扣回报形式。

## 1.3.6 你该如何选？
- 离散、小规模：先上 DQN；若资源有限，表格 Q-learning 也能快速跑通。
- 连续控制：从 PPO（稳定易调）或 SAC（样本效率高）开始；追求简单可先试 DDPG，再切 TD3/SAC。
- 样本极贵：考虑 model-based 或离线 RL（本教程未深入）。
- 计算/调参时间有限：优先选择社区成熟、超参稳的算法（PPO/SAC）。

> 后续章节会按上述分类依次展开，实现可跑的最小示例，帮助你在具体任务中做取舍。
