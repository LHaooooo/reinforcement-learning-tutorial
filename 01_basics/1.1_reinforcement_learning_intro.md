# 1.1 强化学习简介

本节介绍什么是强化学习（Reinforcement Learning, RL），以及 Agent 与环境的交互框架。

## 1.1.1 什么是强化学习

- **核心思想**：智能体（Agent）通过与环境交互，基于奖励信号自行学习决策策略，而无需显式标注的“正确答案”。
- > **目标**：找到一条策略，使得长期累积奖励（回报）最大化。

与监督学习/无监督学习相比：
- 监督：给定输入-标签，最小化误差；
- 无监督：挖掘数据结构（聚类/生成）；
- 强化：只有稀疏、延迟的奖励信号，且当前决策会影响未来的数据分布。

## 1.1.2 经典 Agent-Environment 闭环

时间以离散步 $t=0,1,2,\dots$ 进行：

1. 环境处于状态 $s_t$，反馈给 Agent
2. Agent 根据策略 $\pi$ 选择动作 $a_t$
3. 环境执行动作，转移到新状态 $s_{t+1}$，并给出奖励 $r_t$
4. Agent 根据 $(s_t,a_t,r_t,s_{t+1})$ 更新策略或价值

这形成一个马尔可夫决策过程（MDP），将在 1.4 详细定义。

## 1.1.3 RL 中的几个关键词

- **探索 vs 利用**：既要尝试新动作获取信息（explore），又要利用已知最优动作获取奖励（exploit）。
- **延迟奖励**：某些动作的好坏要若干步后才能体现，需要价值函数或回溯估计。
- **部分可观测**：如果观测不等于真实状态，就进入 POMDP 设定（后续可以用 RNN/信念状态处理）。

## 1.1.4 RL 的常见应用

- 博弈：AlphaGo/AlphaZero（围棋、象棋、扑克）
- 控制与机器人：机械臂操作、平衡车、无人机
- 推荐/广告：序列决策、用户长周期回报
- 系统/调度：资源分配、作业调度、网络拥塞控制

## 1.1.5 本教程的使用方式

- 理论：Markdown 推导直观公式
- 代码：最小可跑示例（CartPole / Pendulum），鼓励读者自行改超参、加噪声、换网络
- 进阶：从表格方法到深度 RL，再到表征与评估，循序渐进
